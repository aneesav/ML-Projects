{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I. Introduction to Scikit-learn\n",
    "\n",
    "Scikit-learn is a commonly used Machine Learning library in Python. It is widely accessible and is notable for its ease of use, largely attributed to the plethora of built in ML packages it possesses. Here, I am going to dive a bit deeper into the fundamentals of ML, and complete a classification project on a toy image dataset. This project will test: ML pipeline build (model selection, instantiation, test/train and prediction/evaluation) as well as classification algorithm use and introductory cross-validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1. Import the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2. Import the `digits` toy dataset from Scikitlearn\n",
    "This is a copy of the test set of the UCI ML hand-written digits datasets https://archive.ics.uci.edu/ml/datasets/Optical+Recognition+of+Handwritten+Digits\n",
    "\n",
    "The dataset contains images of hand-written digits: 10 classes where each class refers to a digit.\n",
    "\n",
    "Each datapoint is a 8x8 image of a digit; Classes=10, Samples per class ~180.\n",
    "\n",
    "Samples total = 1797, Dimensionality = 64, and Features = integers 0-16\n",
    "\n",
    "\n",
    "After importing this dataset, split it into test and train sets. Check the shape of data and the target attributes of the dataset. Print a few samples from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "digits = load_digits()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['data', 'target', 'frame', 'feature_names', 'target_names', 'images', 'DESCR'])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "digits.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y = digits.data, digits.target\n",
    "\n",
    "### Setting data and target features of the digits dataset to x and y variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1797, 64)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape\n",
    "\n",
    "### Our dataset (X) now has 1797 observations and 64 features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  0.,  5., 13.,  9.,  1.,  0.,  0.,  0.,  0., 13., 15., 10.,\n",
       "       15.,  5.,  0.,  0.,  3., 15.,  2.,  0., 11.,  8.,  0.,  0.,  4.,\n",
       "       12.,  0.,  0.,  8.,  8.,  0.,  0.,  5.,  8.,  0.,  0.,  9.,  8.,\n",
       "        0.,  0.,  4., 11.,  0.,  1., 12.,  7.,  0.,  0.,  2., 14.,  5.,\n",
       "       10., 12.,  0.,  0.,  0.,  0.,  6., 13., 10.,  0.,  0.,  0.])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[:20]\n",
    "\n",
    "### Shows us corresponding y values: the second row of x corresponds to 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([178, 182, 177, 183, 181, 182, 181, 179, 174, 180])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(np.bincount(y))\n",
    "\n",
    "### Counts the number of occurrences of each value in y array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64,)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[1].shape\n",
    "\n",
    "### Before we plot this, need to turn it into an 8x8 matrix\n",
    "### 8x8 because 8x8 = 64 pixels (picture consists of 64 pixels total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0., 12., 13.,  5.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., 11., 16.,  9.,  0.,  0.],\n",
       "       [ 0.,  0.,  3., 15., 16.,  6.,  0.,  0.],\n",
       "       [ 0.,  7., 15., 16., 16.,  2.,  0.,  0.],\n",
       "       [ 0.,  0.,  1., 16., 16.,  3.,  0.,  0.],\n",
       "       [ 0.,  0.,  1., 16., 16.,  6.,  0.,  0.],\n",
       "       [ 0.,  0.,  1., 16., 16.,  6.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., 11., 16., 10.,  0.,  0.]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[1].reshape(8,8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7ff73f3fdac0>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.matshow(X[1].reshape(8,8))\n",
    "\n",
    "### Whenever we work with ML models we need to turn the data into numbers that the algorithm can ingest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II. Sklearn API for model training\n",
    "-------------------\n",
    "### Step 1. Import your model class\n",
    "\n",
    "As an example, I will use a `LinearSVC`, a linear support vector classifier. This classifier is imprted from `sklearn.svm` module which includes Support Vector Machine algorithms. I have never used this algorithm before, but knowing every single ML algorithm is less important than knowing their appropriate use cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step2. Instantiate an object and set the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm = LinearSVC()\n",
    "#svm2 = LinearSVC()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3. Fit the model\n",
    "When fitting the model, use the train dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,random_state = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/svm/_base.py:1206: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LinearSVC()"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4. Predict and Evaluate\n",
    "Use the test set for this purpose, for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1347, 64)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape\n",
    "\n",
    "### Shape of train dataset: 1347 observations and 64 features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9, 3, 1, ..., 1, 8, 2])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm.predict(X_train)\n",
    "\n",
    "### Predicted x values in the train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9 3 1 ... 1 8 2]\n"
     ]
    }
   ],
   "source": [
    "print(y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.994060876020787"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm.score(X_train, y_train)\n",
    "\n",
    "### Possible overfitting if train dataset does better than test dataset \n",
    "### Training performance is measured in accuracy\n",
    "### If overfitting occurs, should do additional tests to confirm *not shown here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9533333333333334"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm.score(X_test, y_test)\n",
    "\n",
    "### Test performance measured in accuracy\n",
    "### Always expect a difference between test and train accuracy, but shouldn't be too stark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step5. Try another Algorithm\n",
    "Here I will try the `RandomForestCLassifier` instead, imported from the `sklearn.ensemble` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(n_estimators = 50)\n",
    "\n",
    "### n_estimators = hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(n_estimators=50)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9644444444444444"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf.score(X_test, y_test)\n",
    "\n",
    "### Note: I observe a slightly different accuracy measure in test performance when using a different classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II. Cross-validation\n",
    "\n",
    "Learning the parameters of a prediction function and testing it on the same data is a methodological mistake: a model that would just repeat the labels of the samples that it has just seen would have a perfect score but would fail to predict anything useful on yet-unseen data. This situation is called overfitting. To avoid it, it is common practice when performing a (supervised) machine learning experiment to hold out part of the available data as a test set X_test, y_test.\n",
    "\n",
    "When evaluating different settings (“hyperparameters”) for estimators, there is still a risk of overfitting on the test set because the parameters can be tweaked until the estimator performs optimally. This way, knowledge about the test set can “leak” into the model and evaluation metrics no longer report on generalization performance. To solve this problem, yet another part of the dataset can be held out as a so-called “validation set”: training proceeds on the training set, after which evaluation is done on the validation set, and when the experiment seems to be successful, final evaluation can be done on the test set.\n",
    "\n",
    "However, by partitioning the available data into three sets, we drastically reduce the number of samples which can be used for learning the model, and the results can depend on a particular random choice for the pair of (train, validation) sets.\n",
    "\n",
    "A solution to this problem is a procedure called cross-validation (CV for short). A test set should still be held out for final evaluation, but the validation set is no longer needed when doing CV. In the basic approach, called k-fold CV, the training set is split into k smaller sets (other approaches are described below, but generally follow the same principles). The following procedure is followed for each of the k “folds”:\n",
    "\n",
    "A model is trained using of the folds as training data; the resulting model is validated on the remaining part of the data (i.e., it is used as a test set to compute a performance measure such as accuracy).\n",
    "\n",
    "The performance measure reported by k-fold cross-validation is then the average of the values computed in the loop. This approach can be computationally expensive, but does not waste too much data (as is the case when fixing an arbitrary validation set), which is a major advantage in problems such as inverse inference where the number of samples is very small. (Check https://scikit-learn.org/stable/modules/cross_validation.html#cross-validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = cross_val_score(KNeighborsClassifier(), X_train, y_train, cv = 5)\n",
    "\n",
    "### Note: Cross-validation does not produce a model\n",
    "### Remove \"scores\" alias to see output\n",
    "### cv = how many folds to divide the training data into\n",
    "### retuns an array of 5 scores: X1, X2...X5. I.e the scores of 1/5th of my dataset used for training purposes.\n",
    "### I.e the distribution of scores in 1 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9799614484372849, 0.0037511753624214594)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores.mean(), scores.std()\n",
    "\n",
    "### Not much variation i.e not much overfitting occuring at a glance"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
